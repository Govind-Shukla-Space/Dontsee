"""
Weather Producer - Milestone 1 (PySpark Streaming)
Simulates IoT devices by sending JSON weather data row by row into Kafka topic: weather.readings
"""

import json
from pyspark.sql import SparkSession
from pyspark.sql.functions import expr, to_json, struct

TOPIC = "weather.readings"

if __name__ == "__main__":
    spark = SparkSession.builder \
        .appName("WeatherProducer") \
        .master("local[*]") \
        .getOrCreate()

    spark.sparkContext.setLogLevel("WARN")

    # Load static dataset (all weather records)
    static_df = spark.read.json("data.json")

    # Add a monotonically increasing id column
    static_df = static_df.withColumn("id", expr("monotonically_increasing_id()"))

    # Use Spark "rate" source to simulate streaming ticks (1 row per second)
    rate_df = spark.readStream \
        .format("rate") \
        .option("rowsPerSecond", 1) \
        .load() \
        .withColumnRenamed("value", "id")

    # Join the ticking rate with static dataset on id
    streaming_df = rate_df.join(static_df, "id")

    # Convert row to JSON for Kafka
    kafka_df = streaming_df.select(
        to_json(struct([col for col in static_df.columns])).alias("value")
    )

    # Write to Kafka topic
    query = kafka_df.writeStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "localhost:9092") \
        .option("topic", TOPIC) \
        .option("checkpointLocation", "./chk-weather-producer") \
        .start()

    query.awaitTermination()

consumer
"""
Weather Consumer - Milestone 1 (PySpark version)
Consumes messages from Kafka topic: weather.readings
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col

TOPIC = "weather.readings"

if __name__ == "__main__":
    spark = SparkSession.builder \
        .appName("WeatherConsumer") \
        .master("local[*]") \
        .getOrCreate()

    spark.sparkContext.setLogLevel("WARN")

    # Read stream from Kafka
    df = spark.readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "localhost:9092") \
        .option("subscribe", TOPIC) \
        .option("startingOffsets", "latest") \
        .load()

    # Extract Kafka value (binary â†’ string)
    messages = df.select(col("value").cast("string").alias("weather_message"))

    query = messages.writeStream \
        .outputMode("append") \
        .format("console") \
        .option("truncate", False) \
        .start()

    query.awaitTermination()
