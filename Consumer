"""
Weather Producer
Reads weather readings from data.json
Publishes each record as a message to Kafka topic: weather.readings
"""

import time
import json
from kafka import KafkaProducer

TOPIC = "weather.readings"

if __name__ == "__main__":
    print("ðŸš€ Starting Weather Producer with JSON data...")

    # Initialize Kafka producer
    producer = KafkaProducer(
        bootstrap_servers="localhost:9092",
        value_serializer=lambda v: json.dumps(v).encode("utf-8")
    )

    # Load JSON file
    with open("data.json", "r") as f:
        records = json.load(f)

    for record in records:
        # Send message to Kafka
        producer.send(TOPIC, value=record)
        producer.flush()
        print(f"âœ… Sent: {record}")

        time.sleep(1)  # simulate sensor interval

    print("âœ… Finished sending all records from JSON")
    producer.close()

consumer


"""
Weather Consumer
Consumes weather readings from Kafka
Logs messages to console
Uses PySpark Structured Streaming
"""

from pyspark.sql import SparkSession

TOPIC = "weather.readings"

if __name__ == "__main__":
    print("ðŸš€ Starting Weather Consumer...")

    spark = SparkSession.builder \
        .appName("WeatherConsumer") \
        .master("local[*]") \
        .getOrCreate()

    spark.sparkContext.setLogLevel("WARN")

    # Read from Kafka
    df = spark.readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "localhost:9092") \
        .option("subscribe", TOPIC) \
        .option("startingOffsets", "latest") \
        .load()

    # Extract Kafka "value" as string
    weather_df = df.selectExpr("CAST(value AS STRING) as message")

    # Write to console
    query = weather_df.writeStream \
        .outputMode("append") \
        .format("console") \
        .option("truncate", False) \
        .start()

    query.awaitTermination()
