"""
Weather Producer
Reads weather readings from data.csv
Publishes each row as a message to Kafka topic: weather.readings
"""

import time
import pandas as pd
from kafka import KafkaProducer

TOPIC = "weather.readings"

if __name__ == "__main__":
    print("ðŸš€ Starting Weather Producer with CSV data...")

    # Initialize Kafka producer
    producer = KafkaProducer(
        bootstrap_servers="localhost:9092",
        value_serializer=lambda v: str(v).encode("utf-8")
    )

    # Load CSV
    df = pd.read_csv("data.csv")

    for _, row in df.iterrows():
        record = row.to_dict()
        message = str(record)

        # Send message to Kafka
        producer.send(TOPIC, value=message)
        producer.flush()
        print(f"âœ… Sent: {message}")

        time.sleep(1)  # simulate sensor interval

    print("âœ… Finished sending all records from CSV")
    producer.close()

c9nsumer
"""
Weather Consumer
Consumes weather readings from Kafka
Logs messages to console
Uses PySpark Structured Streaming
"""

from pyspark.sql import SparkSession

TOPIC = "weather.readings"

if __name__ == "__main__":
    print("ðŸš€ Starting Weather Consumer...")

    spark = SparkSession.builder \
        .appName("WeatherConsumer") \
        .master("local[*]") \
        .getOrCreate()

    spark.sparkContext.setLogLevel("WARN")

    # Read from Kafka
    df = spark.readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "localhost:9092") \
        .option("subscribe", TOPIC) \
        .option("startingOffsets", "latest") \
        .load()

    # Extract Kafka "value" field
    weather_df = df.selectExpr("CAST(value AS STRING) as message")

    # Write to console
    query = weather_df.writeStream \
        .outputMode("append") \
        .format("console") \
        .option("truncate", False) \
        .start()

    query.awaitTermination()

