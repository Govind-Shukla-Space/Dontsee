from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType
from etl.validate import validate_sample_t20_match
from etl.transform import transform_sample_t20_match_to_fact_ballbyball, build_player_match_stats_from_ballbyball
from etl.load import write_df_to_jdbc

def run_streaming():
    spark = (
        SparkSession.builder
        .appName("CricketKafkaStreaming")
        .config("spark.jars", "/path/to/mysql-connector-j-9.4.0.jar")  # adjust path
        .getOrCreate()
    )

    # Kafka source config
    kafka_df = (
        spark.readStream
        .format("kafka")
        .option("kafka.bootstrap.servers", "localhost:9092")
        .option("subscribe", "cricket.match.ball")
        .option("startingOffsets", "latest")
        .load()
    )

    # Define schema for ball-by-ball JSON
    ball_schema = StructType([
        StructField("matchId", IntegerType()),
        StructField("inning", IntegerType()),
        StructField("over", IntegerType()),
        StructField("ball", IntegerType()),
        StructField("battingTeamId", IntegerType()),
        StructField("bowlingTeamId", IntegerType()),
        StructField("strikerId", IntegerType()),
        StructField("nonStrikerId", IntegerType()),
        StructField("bowlerId", IntegerType()),
        StructField("ballSpeed", FloatType()),
        StructField("runsBatsman", IntegerType()),
        StructField("runsExtras", IntegerType()),
        StructField("extrasType", StringType()),
        StructField("is_wicket", IntegerType()),
        StructField("wicketKind", StringType()),
        StructField("wicketPlayerOutId", IntegerType()),
        StructField("wicketFielderId", IntegerType())
    ])

    # Parse JSON from Kafka "value" field
    parsed_df = kafka_df.selectExpr("CAST(value AS STRING) as json") \
        .select(from_json(col("json"), ball_schema).alias("data")) \
        .select("data.*")

    # Validate (reusing batch validation if possible)
    clean_df = validate_sample_t20_match(parsed_df)

    # Transform â†’ fact tables
    fact_ballbyball = transform_sample_t20_match_to_fact_ballbyball(clean_df)
    fact_player_stats = build_player_match_stats_from_ballbyball(fact_ballbyball)

    # Write fact_ballbyball to MySQL
    query1 = (
        fact_ballbyball.writeStream
        .foreachBatch(lambda df, epochId: write_df_to_jdbc(df, "fact_ballbyball", mode="append"))
        .outputMode("append")
        .start()
    )

    # Write fact_player_match_stats to MySQL
    query2 = (
        fact_player_stats.writeStream
        .foreachBatch(lambda df, epochId: write_df_to_jdbc(df, "fact_player_match_stats", mode="append"))
        .outputMode("append")
        .start()
    )

    spark.streams.awaitAnyTermination()

if __name__ == "__main__":
    run_streaming()


from kafka import KafkaProducer
import json, time

producer = KafkaProducer(bootstrap_servers="localhost:9092")

sample_event = {
    "matchId": 101,
    "inning": 1,
    "over": 5,
    "ball": 3,
    "battingTeamId": 1,
    "bowlingTeamId": 2,
    "strikerId": 15,
    "nonStrikerId": 22,
    "bowlerId": 30,
    "ballSpeed": 138.5,
    "runsBatsman": 4,
    "runsExtras": 0,
    "extrasType": None,
    "is_wicket": 0,
    "wicketKind": None,
    "wicketPlayerOutId": None,
    "wicketFielderId": None
}

for i in range(20):
    producer.send("cricket.match.ball", json.dumps(sample_event).encode("utf-8"))
    time.sleep(1)

producer.flush()
