pr
import json
import time
import pymysql
from kafka import KafkaProducer
from config.db_config import DB_CONFIG


def get_mysql_connection():
    """Connect to source MySQL DB using pymysql and DB_CONFIG."""
    return pymysql.connect(
        host="localhost",
        port=3306,
        user=DB_CONFIG["user"],
        password=DB_CONFIG["password"],
        database="cricket",   # from DB_CONFIG["source_url"]
        cursorclass=pymysql.cursors.DictCursor
    )


def produce_ball_events():
    producer = KafkaProducer(
        bootstrap_servers="localhost:9092",
        value_serializer=lambda v: json.dumps(v).encode("utf-8")
    )

    conn = get_mysql_connection()
    cursor = conn.cursor()

    # Fetch sample rows from sample_t20_match
    query = """
        SELECT matchId, inning, over, ball, battingTeamId, bowlingTeamId,
               strikerId, nonStrikerId, bowlerId,
               ballSpeed, runsBatsman, runsExtras, extrasType,
               is_wicket, wicketKind, wicketPlayerOutId, wicketFielderId
        FROM sample_t20_match
        LIMIT 100
    """
    cursor.execute(query)
    rows = cursor.fetchall()

    for row in rows:
        producer.send("cricket.match.ball", value=row)
        print(f"Produced event: {row}")
        time.sleep(0.5)  # simulate live feed

    cursor.close()
    conn.close()
    producer.close()


if __name__ == "__main__":
    produce_ball_events()
cons
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col
from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, StringType

from etl.validate import validate_sample_t20_match
from etl.transform import (
    transform_sample_t20_match_to_fact_ballbyball,
    build_player_match_stats_from_ballbyball
)
from etl.load import write_df_to_jdbc


def run_streaming():
    spark = (
        SparkSession.builder
        .appName("CricketKafkaStreamingETL")
        .config("spark.jars", "/spark/jars/mysql-connector-j-9.4.0.jar")
        .getOrCreate()
    )

    # === 1. Kafka Source ===
    kafka_df = (
        spark.readStream
        .format("kafka")
        .option("kafka.bootstrap.servers", "localhost:9092")
        .option("subscribe", "cricket.match.ball")
        .option("startingOffsets", "latest")
        .load()
    )

    # === 2. Schema (must match producer rows) ===
    ball_schema = StructType([
        StructField("matchId", IntegerType()),
        StructField("inning", IntegerType()),
        StructField("over", IntegerType()),
        StructField("ball", IntegerType()),
        StructField("battingTeamId", IntegerType()),
        StructField("bowlingTeamId", IntegerType()),
        StructField("strikerId", IntegerType()),
        StructField("nonStrikerId", IntegerType()),
        StructField("bowlerId", IntegerType()),
        StructField("ballSpeed", FloatType()),
        StructField("runsBatsman", IntegerType()),
        StructField("runsExtras", IntegerType()),
        StructField("extrasType", StringType()),
        StructField("is_wicket", IntegerType()),
        StructField("wicketKind", StringType()),
        StructField("wicketPlayerOutId", IntegerType()),
        StructField("wicketFielderId", IntegerType())
    ])

    parsed_df = (
        kafka_df.selectExpr("CAST(value AS STRING) as json")
        .select(from_json(col("json"), ball_schema).alias("data"))
        .select("data.*")
    )

    # === 3. Validation ===
    clean_df = validate_sample_t20_match(parsed_df)

    # === 4. Transform into target models ===
    fact_ballbyball = transform_sample_t20_match_to_fact_ballbyball(clean_df)
    fact_player_stats = build_player_match_stats_from_ballbyball(fact_ballbyball)

    # === 5a. Sink: Write to cricket_verified DB ===
    query_fact_ball = (
        fact_ballbyball.writeStream
        .foreachBatch(lambda df, epochId: write_df_to_jdbc(df, "fact_ballbyball", "append"))
        .outputMode("append")
        .start()
    )

    query_fact_player = (
        fact_player_stats.writeStream
        .foreachBatch(lambda df, epochId: write_df_to_jdbc(df, "fact_player_match_stats", "append"))
        .outputMode("append")
        .start()
    )

    # === 5b. Sink: Register as in-memory tables for queries ===
    fact_ballbyball.createOrReplaceTempView("live_fact_ballbyball")
    fact_player_stats.createOrReplaceTempView("live_fact_player_match_stats")

    # Example business query
    top_scorers = spark.sql("""
        SELECT playerId, SUM(runsScored) as totalRuns
        FROM live_fact_player_match_stats
        GROUP BY playerId
        ORDER BY totalRuns DESC
        LIMIT 10
    """)

    query_top_scorers = (
        top_scorers.writeStream
        .outputMode("complete")
        .format("memory")
        .queryName("top_scorers_view")
        .start()
    )

    spark.streams.awaitAnyTermination()


if __name__ == "__main__":
    run_streaming()
